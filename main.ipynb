{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3EgnDXKKsKn"
      },
      "source": [
        "**Retrieval Augmented Generation exercise:**\n",
        "- Use the following dataset as an embedded dataset to search from: https://huggingface.co/datasets/wikipedia/viewer/20220301.en\n",
        "- Create a retrieval layer that searches keywords with vector comparisons\n",
        "- Use a trained chat model to generate a natural language response that includes data from the related article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZWCnwLlKsKo"
      },
      "source": [
        "# Data Retrieval\n",
        "\n",
        "Many solutions and frameworks exist for this problem, still I would like to build the system layer by layer.\n",
        "\n",
        "HuggingFace's recommended solution for data retrieval is using [FAISS](https://huggingface.co/learn/nlp-course/chapter5/6?fw=tf)\n",
        "\n",
        "With FAISS, we can supply a trained model with the vectorized,tokenized dataset,\n",
        "and it will generate a large corpus of vectors that can be used to match a query against."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb4XC0O6y-8q"
      },
      "source": [
        "\n",
        "### Requirements:\n",
        "Google colab free tier uses cpu computing so tensorflow-cpu and faiss-cpu is needed.\n",
        "\n",
        "Feel free to change your dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tDFyEzCK9ir"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow-cpu torch torchtext torchdata datasets transformers accelerate faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjrDTY8uKsKq"
      },
      "source": [
        "### Retrieval Layer:\n",
        "\n",
        "After our data has been processed, it has to be converted into vectors\n",
        "\n",
        "This Retrieval Layer class handles the processing of the dataset and finding the best match for a query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2PZYS6vhKsKq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import pandas as pd\n",
        "\n",
        "class RetrievalLayer:\n",
        "\n",
        "    def __init__(self, model_name, dataset):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = TFAutoModel.from_pretrained(model_name, from_pt=True)\n",
        "\n",
        "        def flatten(data):\n",
        "            return {\n",
        "                \"text\":\n",
        "                    \"url:\\n\"+data[\"url\"]\n",
        "                    +\"\\ntitle:\\n\"+data[\"title\"]\n",
        "                    +\"\\ntext:\\n\"+data[\"text\"]\n",
        "            }\n",
        "\n",
        "        dataset = dataset.map(flatten)\n",
        "\n",
        "        self.embedded_dataset = dataset.map(\n",
        "            lambda x: {\"embeddings\": self.get_embeddings(x[\"text\"]).numpy()[0]}\n",
        "        )\n",
        "        self.embedded_dataset.add_faiss_index(column=\"embeddings\")\n",
        "\n",
        "    def get_embeddings(self,text_list):\n",
        "        encoded_input = self.tokenizer(\n",
        "            text_list, padding=True, truncation=True, return_tensors=\"tf\"\n",
        "        )\n",
        "        encoded_input = {k: v for k, v in encoded_input.items()}\n",
        "        model_output = self.model(**encoded_input)\n",
        "        return model_output.last_hidden_state[:, 0]\n",
        "\n",
        "    def get_best_match(self,query):\n",
        "        embedded_query = self.get_embeddings([query]).numpy()\n",
        "        scores, samples = self.embedded_dataset.get_nearest_examples(\n",
        "            \"embeddings\", embedded_query, k=5\n",
        "        )\n",
        "        samples_df = pd.DataFrame.from_dict(samples)\n",
        "        samples_df[\"scores\"] = scores\n",
        "        samples_df.sort_values(\"scores\", ascending=True, inplace=True) # Faiss scores are distance based, meaning a lower score is better. The huggingface tutorial incorrectly sorts by Descending order\n",
        "        best_match = samples_df.iloc[0] # Contains the five best matches, we only need the first\n",
        "        return best_match"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXn2vnqczDQ4"
      },
      "source": [
        "### Answer Generation Layer\n",
        "\n",
        "HuggingFace's transformers library makes it very easy to import and use models locally.\n",
        "\n",
        "The [TinyLlama](https://github.com/jzhang38/TinyLlama) chat model is a very small model that can quickly evaluate simple prompts, which makes it ideal for this purpose.\n",
        "\n",
        "We just need to form a sentence from the answer which does not require big models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qELS1YMgKsKq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "def process_question(model_pipeline, question, answer):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Respond to the user's question with the provided article from wikipedia:\\n\" + answer,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": question\n",
        "        }\n",
        "    ]\n",
        "    prompt = model_pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    outputs = model_pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "    return outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNFsBRtB4Rtb"
      },
      "source": [
        "### Usage\n",
        "\n",
        "After all parts of the pipeline have been initalized, we can try it out using the simple functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N6QSblO4KsKp"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Dataset structure:\n",
        "# {\n",
        "#     \"id\": \"1\",\n",
        "#     \"url\": \"https://simple.wikipedia.org/wiki/April\",\n",
        "#     \"title\": \"April\",\n",
        "#     \"text\": \"April is the fourth month of the year\"\n",
        "# }\n",
        "\n",
        "#dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", trust_remote_code=True)\n",
        "\n",
        "# A small portion of the dataset used for development (free tier google colab can process this relatively quickly):\n",
        "full_dataset = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train\", trust_remote_code=True)\n",
        "dataset = full_dataset.shard(num_shards=200, index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the trained chat model\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytaTdJbENAzR"
      },
      "outputs": [],
      "source": [
        "# Create the vectorized dataset\n",
        "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "retrieval_layer = RetrievalLayer(model_name,dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "K3IJpTPJ3N3E"
      },
      "outputs": [],
      "source": [
        "# Pass a question to find it's best match\n",
        "question = \"Who is Snorri Sturluson?\"\n",
        "best_match = retrieval_layer.get_best_match(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Dp3CAMyEZW",
        "outputId": "ca07bb91-e943-444e-be81-6fb0e26e5911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Snorri Sturluson was an Icelandic historian, poet, and politician who lived in the late 12th and early 13th centuries. He is best known for his three-volume work, the Prose Edda, which is a collection of myths, legends, and historical tales from Norse mythology. He also wrote the Heimskringla, which is a history of the Norwegian kings that begins with legendary material and moves through to early medieval Scandinavian history. Snorri Sturluson was the author of the Prose Edda and the Heimskringla, which have been translated into numerous languages and continue to be studied and read today.\n"
          ]
        }
      ],
      "source": [
        "# Generate the answer from the question and the related wikipedia article\n",
        "answer = process_question(pipe,question,best_match.text)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8o-SGfn_QRQ"
      },
      "source": [
        "# Results\n",
        "\n",
        "Evaluating the performance of the pipeline in-depth would require more resources, so for this exercise I will only suggest some methods to evaluate the pipeline.:\n",
        "\n",
        "- [RAGAS](https://docs.ragas.io/en/stable/howtos/customisations/llms.html) - is a tool specifically created for evaluating RAG pipelines, you need to store the question, the context, and the generated answer in a dataset, and a trained large language model will assign a score to the generated answers.\n",
        "- [PromptFoo](https://www.promptfoo.dev/docs/guides/evaluate-rag/) - is another tool that can evaluate RAG pipelines, although this tool is more general and offers functionality for evaluating other types of LLM's as well.\n",
        "- User Feedback - the most important metric should be user satisfaction, that can be easily collected automatically after a user requests an answer. This will produce real life scenarios that you can't always prepare for.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWnGTeux_Sdc"
      },
      "source": [
        "# Deployment\n",
        "\n",
        "If the load is very high, the best method for deployment would be using a microservice architecture, like the following:\n",
        "\n",
        "![](.docs/graph.png)\n",
        "\n",
        "The RetrievalLayer processes the question quickly, while the language generation takes more time to finish. With this architecture we can easily scale the retrieval and the llm layers, with a different number of api's to handle the load, similar to a producer-consumer pattern.\n",
        "\n",
        "This method is of course overkill, and a simple monolithic api could even perform better, if we do not need quick responses globally.\n",
        "\n",
        "Still, separating the main API that the user can access, and the RAG pipeline is the ideal solution. This way generating the answers won't slow down the content server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNrcEevt6scv"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "This is a very simple locally hosted solution for the RAG Pipeline exercise, that can easily be modified and extended. It can handle any kind of dataset with minimal modifications, and uses small amount of resources to generate answers, so it can be easily scaled as well.\n",
        "\n",
        "Complete solutions exist as services however, for example:\n",
        "\n",
        "- [Chroma](https://www.trychroma.com/)\n",
        "- [Microsoft Cognitive Search](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/revolutionize-your-enterprise-data-with-chatgpt-next-gen-apps-w/ba-p/3762087)\n",
        "- [Elastic Search Relevance Engine](https://www.elastic.co/elasticsearch/machine-learning)\n",
        "- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)\n",
        "\n",
        "Some other open-source, self-hosted solutions:\n",
        "\n",
        "- [Canopy](https://github.com/pinecone-io/canopy)\n",
        "- [Verba](https://github.com/weaviate/Verba)\n",
        "- [RepoChat](https://github.com/pnkvalavala/repochat) - *Specifically for github repositories*\n",
        "\n",
        "And many, many more that are not listed. All of these programs solve the problem in slightly different manners, but the basic idea is the same for all of them."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "kpmg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
